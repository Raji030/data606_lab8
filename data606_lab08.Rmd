---
title: "Data606_lab08"
author: "Mahmud Hasan Al Raji"
date: "`r Sys.Date()`"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Introduction to linear regression
In this lab, I will analyze data from Human Freedom Index reports from 2008-2016.The Human Freedom Index report attempts to summarize the idea of “freedom” through a bunch of different variables for many countries around the globe. It serves as a rough objective measure for the relationships between the different types of freedom - whether it’s political, religious, economical or personal freedom - and other social and economic circumstances.  My aim is to summarize a few of the relationships within the data both graphically and numerically in order to find variables that can help tell a story about freedom.

# Loading packages
```{r }
library(tidyverse)
library(openintro)
library(ggplot2)
library(dplyr)
```

# Load data
```{r }
data('hfi', package='openintro')
```

# View data dimension
```{r }
dim(hfi)
```
# Exercise 1: What are the dimensions of the dataset?

**Ans 1: The data set dimensions are 1458 rows and 123 columns. 

# Exercise 2: What type of plot would you use to display the relationship between the personal freedom score, pf_score, and one of the other numerical variables? Plot this relationship using the variable pf_expression_control as the predictor. Does the relationship look linear? If you knew a country’s pf_expression_control, or its score out of 10, with 0 being the most, of political pressures and controls on media content, would you be comfortable using a linear model to predict the personal freedom score?

** Ans 2: I would like to plot a scatter plot to graphically represent relationship between the personal freedom score, pf_score and one of the other numerical variables. Considerig pf_expression_control as the predictor (independent variable), the plot of pf_expression_control vs pf_score is given below: 

```{r}
hfi<-data.frame(hfi)
hfi_plot<-hfi %>% filter(!is.na(pf_score)&!is.na(pf_expression_control))
  ggplot(hfi_plot,aes(x=pf_expression_control,y=pf_score))+geom_point(stat="identity")

```
It is seen above that the relationship between pf_expression_control and pf_score is linear. So, I would be comfortable to use a linear regression model here.

# Find correlation coefficient
```{r }
hfi %>%
  summarise(cor(pf_expression_control, pf_score, use = "complete.obs"))
```

# Sum of squared residuals
I will use an interactive function to investigate what we mean by “sum of squared residuals”. It is useful to describe the relationship of two numerical variables, such as pf_expression_control and pf_score above.

# Exercise 3: Looking at your plot from the previous exercise, describe the relationship between these two variables. Make sure to discuss the form, direction, and strength of the relationship as well as any unusual observations.

** Ans 3: The plot from the previous exercise shows a linear relationship between the variables. The line's trend is positively upward. Also, from the value of the correlation coefficient above (>0.7), it can be said the relationship between the two variables is strong. 

```{r}
hfi_n <- drop_na(hfi[c('pf_score','pf_expression_control')])
DATA606::plot_ss(x = hfi_n$pf_expression_control, y = hfi_n$pf_score)
```

The most common way to do linear regression is to select the line that minimizes the sum of squared residuals. To visualize the squared residuals, I can rerun the plot command and add the argument showSquares = TRUE. It is noted that the output from the plot_ss function provides us with the slope and intercept of the line as well as the sum of squares.

```{r }
DATA606::plot_ss(x = hfi_n$pf_expression_control, y = hfi_n$pf_score, showSquares = TRUE)
```

# Exercise 4: Using plot_ss, choose a line that does a good job of minimizing the sum of squares. Run the function several times. What was the smallest sum of squares that you got? How does it compare to your neighbors?

** Ans 4: The smallest sum of squares I got was 952.153.

# The linear model
It is rather cumbersome to try to get the correct least squares line, i.e. the line that minimizes the sum of squared residuals, through trial and error. Instead, I can use the lm function in R to fit the linear model (a.k.a. regression line).

```{r }
m1 <- lm(pf_score ~ pf_expression_control, data = hfi)
summary(m1)
```
With this table, we can write down the least squares regression line for the linear model:

y^=4.61707+0.49143×pf_expression_control

One last piece of information we will discuss from the summary output is the Multiple R-squared, or more simply, R2. The R2 value represents the proportion of variability in the response variable that is explained by the explanatory variable. For this model, 63.42% of the variability in runs is explained by at-bats.```

# Exercise 5: Fit a new model that uses pf_expression_control to predict hf_score, or the total human freedom score. Using the estimates from the R output, write the equation of the regression line. What does the slope tell us in the context of the relationship between human freedom and the amount of political pressure on media content?

```{r }
m2 <- lm(hf_score ~ pf_expression_control, data = hfi)
summary(m2)
```
** Ans 5: From the R output above, the equation of the regression line for the linear model to predict hf_score is hf_score = 5.153687 + 0.349862*pf_expression_control. In the context of the relationship between human freedom and the amount of political pressure on media content, the slope is telling us that the amount of change in human freedom value caused by the unit change in political pressure on media content is 0.349862. 


# Prediction and prediction errors
Let’s create a scatter plot with the least squares line for m1 laid on top.

```{r }
ggplot(data = hfi, aes(x = pf_expression_control, y = pf_score)) +
  geom_point() +
  stat_smooth(method = "lm", se = FALSE)
```

# Exercise 6 : If someone saw the least squares regression line and not the actual data, how would they predict a country’s personal freedom school for one with a 6.7 rating for pf_expression_control? Is this an overestimate or an underestimate, and by how much? In other words, what is the residual for this prediction?

** Ans 6: From the plot above it is found that the predicted country's personal freedom score is approximately 8 at 6.7 rating for pf_expression_control. The predicted value can be estimate from the linear regression equation written previously: 

```{r }
4.61707 + 0.49143*6.7
```
So, the predicted value found from the equation is also nearly 8. Let's find the actual value from the observed data:

```{r }
hfi %>% select(pf_score,pf_expression_control) %>% filter(pf_expression_control>=6.6 & pf_expression_control<=6.8) %>% group_by(pf_expression_control) %>% summarise(mean_score=mean(pf_score),median_score=median(pf_score))
```                                                                                                                                                       

It is also seen that the predicted value is nearly close to the center of the nearest observed value.

# Model diagnostics
To assess whether the linear model is reliable, we need to check for (1) linearity, (2) nearly normal residuals, and (3) constant variability.

Linearity: We already checked if the relationship between pf_score and `pf_expression_control’ is linear using a scatter plot. We should also verify this condition with a plot of the residuals vs. fitted (predicted) values.

```{r }
ggplot(data = m1, aes(x = .fitted, y = .resid)) +
  geom_point() +
  geom_hline(yintercept = 0, linetype = "dashed") +
  xlab("Fitted values") +
  ylab("Residuals")
```
Notice here that m1 can also serve as a data set because stored within it are the fitted values (y^) and the residuals. Also note that we’re getting fancy with the code here. After creating the scatterplot on the first layer (first line of code), we overlay a horizontal dashed line at y=0 (to help us check whether residuals are distributed around 0), and we also rename the axis labels to be more informative.

# Exercise 7: Is there any apparent pattern in the residuals plot? What does this indicate about the linearity of the relationship between the two variables?

** Ans 7: From the plot above, it is seen that the residuals are distributed around 0 with a consistent pattern which indicates a linear trend. So, we can assume a linear regression model for fitting the data set here.

Nearly normal residuals: To check this condition, we can look at a histogram:

```{r }
ggplot(data = m1, aes(x = .resid)) +
  geom_histogram(binwidth = 0.5) +
  xlab("Residuals")
```

or a normal probability plot of the residuals:

```{r }
ggplot(data = m1, aes(sample = .resid)) +
  stat_qq()
```
Note that the syntax for making a normal probability plot is a bit different than what you’re used to seeing: we set sample equal to the residuals instead of x, and we set a statistical method qq, which stands for “quantile-quantile”, another name commonly used for normal probability plots.

# Exercise 8: Based on the histogram and the normal probability plot, does the nearly normal residuals condition appear to be met?

** Ans 8: It is seen that the histogram is nearly normally distributed (slight left-skewed). Also, the qq plot is very close to a straight line. So, the residuals condition for being nearly normal is met here.

Constant variability:

# Exercise 9: Based on the residuals vs. fitted plot, does the constant variability condition appear to be met?

** Ans 9: From the residuals vs. fitted plot above, it is seen that the spread of the residuals is roughly equal at each level of the fitted values. So, we can say that the constant variance condition or assumption is met here.

# More Practice

# Exercise 10: Choose another freedom variable and a variable you think would strongly correlate with it.. Produce a scatterplot of the two variables and fit a linear model. At a glance, does there seem to be a linear relationship?

** Ans 10: I chose pf_religion_restrictions and pf_score data to understand the relationship between them. After plotting a scatter plot it looks like there is a linear relationship between the two variables.

```{r }
hfi_1 <- drop_na(hfi[c('pf_religion_restrictions','pf_score')])
DATA606::plot_ss(x = hfi_1$pf_religion_restrictions, y = hfi_1$pf_score)
```

# Exercise 11: How does this relationship compare to the relationship between pf_expression_control and pf_score? Use the R2 values from the two model summaries to compare. Does your independent variable seem to predict your dependent one better? Why or why not?

```{r }
m3 <- lm(pf_score ~ pf_religion_restrictions, data = hfi)
summary(m3)
```
** Ans 11: In general, the higher the R-squared, the better the model fits the data. The R-squared value for the pf_expression_control and pf_score relationship model was 0.634. And the R-squared value for the pf_religion_restrictions and pf_score relationship model is 0.04177. So, the previous model will explain 59.22% more of the fitted data in regression model. Hence, it can be said that the pf_expression_control and pf_score relationship model is the best fitted model compared to the model for the pf_religion_restrictions and pf_score relationship. Hence, my independent variable here does not seem to predict the dependent variable better than the previous one.

Creating a scatter plot for the above two variables with the least squares line for m3 laid on top:

```{r }
ggplot(data = hfi_1, aes(x = pf_religion_restrictions, y = pf_score)) +
  geom_point() +
  stat_smooth(method = "lm", se = FALSE)
```

# Exercise 12: What’s one freedom relationship you were most surprised about and why? Display the model diagnostics for the regression model analyzing this relationship.

** Ans 12: I am surprised to see here that there is no linear relationship between the personal freedom score and religion restrictions. I thought countries that have low personal freedom score have some restrictions on religious freedom.

My linear model's diagnostics will be done here based on 3 conditions: (1) Linearity (2) Nearly normal residuals (3) Constant variability.

Linearity check: Verifying the linearity condition with a plot of the residuals vs. fitted (predicted) values:

```{r }

ggplot(data = m3, aes(x = .fitted, y = .resid)) +
  geom_point() +
  geom_hline(yintercept = 0, linetype = "dashed") +
  xlab("Fitted values") +
  ylab("Residuals")
```
From the plot above, it is seen that the residuals are not distributed around 0 with a consistent pattern which indicates a non-linear trend. So, we can not assume a linear regression model for best fitting the data set here.

Nearly normal residuals check: To check this condition, we can look at a histogram:

```{r }
ggplot(data = m3, aes(x = .resid)) +
  geom_histogram(binwidth = 0.5) +
  xlab("Residuals")
```

or a normal probability plot of the residuals:

```{r}
ggplot(data = m3, aes(sample = .resid)) +
  stat_qq()
```
It is seen that the histogram is not nearly normally distributed. Also, the qq plot is not very close to a straight line. So, the residuals condition for being nearly normal is not met here.

Constant variability check: From the residuals vs. fitted plot above, it is seen that the spread of the residuals is not roughly equal at each level of the fitted values.There is a lot of clustering of the residuals as the fitted values increase, violating the condition of constant variance. So, we can say that the constant variance condition or assumption is not met here. Therefore, from the diagnostics above it can be said that the linear regression model will not properly predict the relationship between the two variables i chose here. Thus, the personal freedom on religion restrictions can not be the predictor of the personal freedom score. 


